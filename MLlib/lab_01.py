
# Import necessary Spark modules
from pyspark.sql import SparkSession          # Main entry point for Spark
from pyspark.ml.feature import VectorAssembler # Combines multiple columns into one vector
from pyspark.ml.regression import LinearRegression # Linear regression algorithm
from pyspark.ml.evaluation import RegressionEvaluator # Evaluates model performance

# Start Spark in local mode
spark = SparkSession.builder \
    .appName("simple ML") \
    .master("local[*]") \
    .getOrCreate()                           
    
print("âœ… Spark session created successfully!")
print(f"Spark version: {spark.version}")    

# Create sample house data manually (in real world, you'd load from CSV/database)
# Format: (area_sqft, bedrooms, bathrooms, age_years, price_dollars)
data = spark.createDataFrame([
    (1500, 2, 1, 10, 300000),    # Small house, older
    (2000, 3, 2, 5, 450000),     # Medium house, newer
    (1200, 1, 1, 15, 250000),    # Tiny house, old
    (2500, 4, 3, 2, 600000),     # Large house, very new
    (1800, 2, 2, 8, 380000),     # Medium house
    (1600, 2, 1, 12, 320000),    # Small-medium house
    (2200, 3, 2, 6, 480000),     # Large house, newish
    (1000, 1, 1, 20, 200000)     # Very small, very old
], ["area", "bedrooms", "bathrooms", "age", "price"])  # Column names



print("\nğŸ“Š Sample House Data:")
data.show()  # Display the DataFrame in tabular format

print("\nğŸ” Data Schema (column types):")
data.printSchema()

print(f"\nğŸ“ˆ Total houses in dataset: {data.count()}")



print("ğŸ”§ Starting Feature Engineering...")

# ML algorithms expect ALL input features in a SINGLE column as a vector
# VectorAssembler combines multiple columns into one "features" vector
assembler = VectorAssembler(
    inputCols=["area", "bedrooms", "bathrooms", "age"],  # Input: separate columns
    outputCol="features"                                  # Output: combined vector column
)

print("âœ… VectorAssembler created!")
print("Input columns:", assembler.getInputCols())
print("Output column:", assembler.getOutputCol())

# Transform the data - this adds the "features" column
# Original columns remain, new "features" column is added
feature_data = assembler.transform(data)

print("\nğŸ“Š Data with Features Vector:")
feature_data.select("features", "price").show()
# Output will look like: features=[1500.0, 2.0, 1.0, 10.0], price=300000

print("\nğŸ” Full data with all columns:")
feature_data.show()

print("\nğŸ“‹ What happened:")
print("âœ… Combined 4 separate columns into 1 vector column")
print("âœ… Each row now has a 'features' vector: [area, bedrooms, bathrooms, age]")
print("âœ… ML algorithms can now use this vector as input")
print("âœ… Original columns are still there (not deleted)")

# Let's examine one specific row to understand the vector
first_row = feature_data.first()
print(f"\nğŸ” Example - First house:")
print(f"   Original: area={first_row['area']}, bedrooms={first_row['bedrooms']}, bathrooms={first_row['bathrooms']}, age={first_row['age']}")
print(f"   Vector: {first_row['features']}")
print(f"   Price: ${first_row['price']:,}")


print("âœ‚ï¸ Starting Train-Test Split...")

# Split data randomly: 80% for training, 20% for testing
# seed=42 ensures we get the same split every time (reproducible results)
train_data, test_data = feature_data.randomSplit([0.8, 0.2], seed=42)

# Check how many samples we got in each set
train_count = train_data.count()
test_count = test_data.count()
total_count = feature_data.count()

print(f"ğŸ“Š Data Split Results:")
print(f"   Total samples: {total_count}")
print(f"   Training samples: {train_count} ({train_count/total_count*100:.1f}%)")
print(f"   Test samples: {test_count} ({test_count/total_count*100:.1f}%)")

print(f"\nğŸ  Training Data (what the model will learn from):")
train_data.select("features", "price").show()

print(f"\nğŸ” Test Data (what we'll use to check accuracy):")
test_data.select("features", "price").show()

print("\nğŸ“‹ Why do we split the data?")
print("âœ… TRAINING SET: Model learns patterns from this data")
print("âœ… TEST SET: We check if model works on UNSEEN data") 
print("âœ… This prevents overfitting (memorizing instead of learning)")
print("âœ… Gives us honest estimate of real-world performance")

print("\nğŸ’¡ The Golden Rule:")
print("ğŸš« NEVER let the model see test data during training!")
print("âœ… Test data must stay 'unseen' until final evaluation")

# Let's see what houses ended up in each set
print(f"\nğŸ” Training set price range: ${train_data.agg({'price': 'min'}).collect()[0][0]:,} to ${train_data.agg({'price': 'max'}).collect()[0][0]:,}")
print(f"ğŸ” Test set price range: ${test_data.agg({'price': 'min'}).collect()[0][0]:,} to ${test_data.agg({'price': 'max'}).collect()[0][0]:,}")



print("ğŸ§  Starting Model Training...")

# Create a Linear Regression model
lr = LinearRegression(
    featuresCol="features",  # Input: the vector column we created
    labelCol="price"         # Output: what we want to predict
)

print("âœ… Linear Regression model created!")
print(f"   Features column: {lr.getFeaturesCol()}")
print(f"   Label column: {lr.getLabelCol()}")

# Train the model using ONLY the training data
# .fit() learns the relationship between features and price
print("\nğŸ”„ Training the model...")
model = lr.fit(train_data)

print("âœ… Model training complete!")

# The model learns coefficients (weights) for each feature
coefficients = model.coefficients
intercept = model.intercept

print(f"\nğŸ“Š What the model learned:")
print(f"   Coefficients: {[round(c, 2) for c in coefficients]}")
print(f"   Intercept: {round(intercept, 2)}")

# Let's interpret what these numbers mean
feature_names = ["area", "bedrooms", "bathrooms", "age"]
print(f"\nğŸ” Model interpretation:")
for i, (name, coef) in enumerate(zip(feature_names, coefficients)):
    direction = "increases" if coef > 0 else "decreases"
    print(f"   {name}: {coef:.2f} -> Each unit {direction} price by ${abs(coef):,.2f}")

print(f"   Base price (intercept): ${intercept:,.2f}")

# Show the learned equation
print(f"\nğŸ“ The model's price prediction formula:")
equation_parts = []
for i, (name, coef) in enumerate(zip(feature_names, coefficients)):
    sign = "+" if coef >= 0 else ""
    equation_parts.append(f"{sign}{coef:.1f} Ã— {name}")

equation = f"price = {' '.join(equation_parts)} + {intercept:.1f}"
print(f"   {equation}")

print(f"\nğŸ’¡ Example prediction calculation:")
print(f"   For a house: 1800 sqft, 3 bed, 2 bath, 5 years old")
example_features = [1800, 3, 2, 5]
manual_prediction = sum(c * f for c, f in zip(coefficients, example_features)) + intercept
print(f"   Predicted price = {coefficients[0]:.1f}Ã—1800 + {coefficients[1]:.1f}Ã—3 + {coefficients[2]:.1f}Ã—2 + {coefficients[3]:.1f}Ã—5 + {intercept:.1f}")
print(f"   Predicted price = ${manual_prediction:,.2f}")




print("ğŸ”® Starting Predictions...")

# Use the trained model to predict prices on test data
# .transform() applies the model to new data and adds a "prediction" column
print("ğŸ”„ Applying model to test data...")
predictions = model.transform(test_data)

print("âœ… Predictions complete!")

print("\nğŸ“Š Predictions vs Actual Prices:")
predictions.select("features", "price", "prediction").show()

# Let's make the comparison easier to read
print("\nğŸ” Detailed Comparison:")
prediction_results = predictions.select("features", "price", "prediction").collect()

for i, row in enumerate(prediction_results):
    features = row['features']
    actual = row['price']
    predicted = row['prediction']
    error = abs(actual - predicted)
    error_percent = (error / actual) * 100
    
    print(f"\nğŸ  House {i+1}:")
    print(f"   Features: {[int(f) for f in features]} (area, bed, bath, age)")
    print(f"   Actual price: ${actual:,.2f}")
    print(f"   Predicted price: ${predicted:,.2f}")
    print(f"   Prediction error: ${error:,.2f} ({error_percent:.1f}%)")
    
    if error_percent < 10:
        print("   âœ… Excellent prediction!")
    elif error_percent < 20:
        print("   ğŸ‘ Good prediction!")
    elif error_percent < 30:
        print("   âš ï¸ Fair prediction")
    else:
        print("   âŒ Poor prediction")

print(f"\nğŸ“‹ What happened behind the scenes:")
print("âœ… Model took each house's features vector")
print("âœ… Applied the learned formula: coef1Ã—area + coef2Ã—bedrooms + ... + intercept")
print("âœ… Generated a predicted price")
print("âœ… Added 'prediction' column to the DataFrame")

print(f"\nğŸ’¡ Key insights:")
if len(prediction_results) > 0:
    avg_error = sum(abs(row['price'] - row['prediction']) for row in prediction_results) / len(prediction_results)
    print(f"   Average prediction error: ${avg_error:,.2f}")
    
    total_actual = sum(row['price'] for row in prediction_results)
    avg_actual = total_actual / len(prediction_results)
    print(f"   Average house price in test set: ${avg_actual:,.2f}")
    
    print(f"   Model accuracy: {100 - (avg_error/avg_actual*100):.1f}%")

print(f"\nğŸš€ This is how you'd use the model in real life:")
print("âœ… New house comes on market")
print("âœ… Input its features [area, bedrooms, bathrooms, age]")
print("âœ… Model instantly predicts the price")
print("âœ… No need to retrain - just apply the learned formula!")




print("ğŸ“ Starting Model Evaluation...")

# Calculate RMSE (Root Mean Squared Error)
# RMSE measures average prediction error in same units as target (dollars)
rmse_evaluator = RegressionEvaluator(
    labelCol="price",           # Actual values
    predictionCol="prediction", # Predicted values  
    metricName="rmse"          # Type of error metric
)

rmse = rmse_evaluator.evaluate(predictions)
print(f"ğŸ“Š Root Mean Squared Error (RMSE): ${rmse:,.2f}")

# Calculate R-squared (coefficient of determination)
# RÂ² tells us how much of price variation our model explains (0 to 1, higher is better)
r2_evaluator = RegressionEvaluator(
    labelCol="price", 
    predictionCol="prediction", 
    metricName="r2"
)

r2 = r2_evaluator.evaluate(predictions)
print(f"ğŸ“Š R-squared (RÂ²): {r2:.3f}")

# Calculate MAE (Mean Absolute Error)
mae_evaluator = RegressionEvaluator(
    labelCol="price", 
    predictionCol="prediction", 
    metricName="mae"
)

mae = mae_evaluator.evaluate(predictions)
print(f"ğŸ“Š Mean Absolute Error (MAE): ${mae:,.2f}")

print(f"\nğŸ” What these metrics mean:")

print(f"\nğŸ“ˆ RMSE (${rmse:,.2f}):")
print(f"   â€¢ Average prediction error in dollars")
print(f"   â€¢ Lower is better (perfect model = $0)")
print(f"   â€¢ Penalizes large errors more heavily")
if rmse < 50000:
    print(f"   âœ… Excellent! Predictions are very accurate")
elif rmse < 100000:
    print(f"   ğŸ‘ Good! Reasonable prediction accuracy")
elif rmse < 150000:
    print(f"   âš ï¸ Fair. Model needs improvement")
else:
    print(f"   âŒ Poor. Model is not very accurate")

print(f"\nğŸ“Š RÂ² ({r2:.3f}):")
print(f"   â€¢ How much price variation the model explains")
print(f"   â€¢ Range: 0.0 (terrible) to 1.0 (perfect)")
print(f"   â€¢ Current: Model explains {r2*100:.1f}% of price differences")
if r2 > 0.9:
    print(f"   âœ… Excellent! Model captures almost all patterns")
elif r2 > 0.7:
    print(f"   ğŸ‘ Good! Model captures most patterns")
elif r2 > 0.5:
    print(f"   âš ï¸ Fair. Model captures some patterns")
else:
    print(f"   âŒ Poor. Model doesn't capture patterns well")

print(f"\nğŸ’° MAE (${mae:,.2f}):")
print(f"   â€¢ Average absolute error (ignores +/- direction)")
print(f"   â€¢ More intuitive than RMSE")
print(f"   â€¢ On average, predictions are off by ${mae:,.2f}")

# Compare RMSE vs MAE to understand error distribution
if rmse > mae * 1.5:
    print(f"\nâš ï¸ RMSE much larger than MAE suggests some very large errors")
else:
    print(f"\nâœ… RMSE close to MAE suggests consistent error sizes")

# Calculate some additional insights
prediction_data = predictions.collect()
if prediction_data:
    actual_prices = [row['price'] for row in prediction_data]
    predicted_prices = [row['prediction'] for row in prediction_data]
    
    avg_actual = sum(actual_prices) / len(actual_prices)
    avg_predicted = sum(predicted_prices) / len(predicted_prices)
    
    print(f"\nğŸ  Price Analysis:")
    print(f"   Average actual price: ${avg_actual:,.2f}")
    print(f"   Average predicted price: ${avg_predicted:,.2f}")
    print(f"   Prediction bias: ${avg_predicted - avg_actual:,.2f}")
    
    if abs(avg_predicted - avg_actual) < avg_actual * 0.05:
        print(f"   âœ… Model is well-calibrated (no systematic bias)")
    else:
        bias_direction = "over-estimating" if avg_predicted > avg_actual else "under-estimating"
        print(f"   âš ï¸ Model is {bias_direction} prices on average")

print(f"\nğŸ¯ Model Performance Summary:")
print(f"   â€¢ Typical error: Â±${mae:,.0f}")
print(f"   â€¢ Explains {r2*100:.0f}% of price variation")
print(f"   â€¢ RMSE: ${rmse:,.0f}")

if r2 > 0.8 and rmse < 50000:
    print(f"   ğŸ† EXCELLENT model - ready for production!")
elif r2 > 0.6 and rmse < 100000:
    print(f"   ğŸ‘ GOOD model - useful for predictions")
elif r2 > 0.4:
    print(f"   âš ï¸ FAIR model - needs improvement")
else:
    print(f"   âŒ POOR model - need more data or different approach")

print(f"\nğŸ’¡ Next steps to improve model:")
print(f"   â€¢ Add more features (location, garage, etc.)")
print(f"   â€¢ Collect more training data")
print(f"   â€¢ Try different algorithms (Random Forest, etc.)")
print(f"   â€¢ Feature engineering (area per bedroom, etc.)")




print("ğŸ”® Predicting Price for a New House...")

# Scenario: A new house just came on the market!
# We want to estimate its price using our trained model

print("\nğŸ  New house details:")
print("   Area: 1900 sq ft")
print("   Bedrooms: 3")
print("   Bathrooms: 2") 
print("   Age: 7 years")


# Create data for the brand new house (not in our training/test data)
new_house = spark.createDataFrame([
    (1900, 3, 2, 7)  # area, bedrooms, bathrooms, age
], ["area", "bedrooms", "bathrooms", "age"])


print("\nğŸ“Š New house as DataFrame:")
new_house.show()

# Apply the SAME feature engineering we used during training
# This is crucial - new data must be processed identically to training data
print("\nğŸ”§ Applying feature engineering...")
new_house_features = assembler.transform(new_house)

print("âœ… Features vector created:")
new_house_features.select("features").show()

# Make prediction using our trained model
print("\nğŸ”„ Making prediction...")
new_prediction = model.transform(new_house_features)

print("âœ… Prediction complete!")
new_prediction.select("area", "bedrooms", "bathrooms", "age", "features", "prediction").show()

# Extract the predicted price from the DataFrame
predicted_price = new_prediction.select("prediction").collect()[0][0]

print(f"\nğŸ’° PREDICTION RESULT:")
print(f"   ğŸ  House: 1900 sq ft, 3 bed, 2 bath, 7 years old")
print(f"   ğŸ’µ Predicted price: ${predicted_price:,.2f}")

# Let's also show the manual calculation to understand what happened
coefficients = model.coefficients
intercept = model.intercept
features = [1900, 3, 2, 7]

print(f"\nğŸ§® Manual calculation (to verify):")
calculation_parts = []
total = intercept
print(f"   Base price (intercept): ${intercept:,.2f}")

feature_names = ["area", "bedrooms", "bathrooms", "age"]
for i, (name, feature_val, coef) in enumerate(zip(feature_names, features, coefficients)):
    contribution = coef * feature_val
    total += contribution
    sign = "+" if contribution >= 0 else ""
    print(f"   {name}: {coef:.2f} Ã— {feature_val} = {sign}${contribution:,.2f}")

print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"   Total predicted price: ${total:,.2f}")
print(f"   (Should match Spark result: ${predicted_price:,.2f})")

# Compare with similar houses from our training data
print(f"\nğŸ” Comparison with similar houses from training:")
similar_houses = train_data.filter(
    (train_data.area.between(1700, 2100)) &  # Similar size
    (train_data.bedrooms == 3)                # Same bedrooms
).select("area", "bedrooms", "bathrooms", "age", "price")

if similar_houses.count() > 0:
    print("   Similar houses in training data:")
    similar_houses.show()
    
    avg_similar_price = similar_houses.agg({"price": "avg"}).collect()[0][0]
    print(f"   Average price of similar houses: ${avg_similar_price:,.2f}")
    print(f"   Our prediction: ${predicted_price:,.2f}")
    difference = predicted_price - avg_similar_price
    print(f"   Difference: ${difference:,.2f}")
else:
    print("   No very similar houses in training data")

print(f"\nğŸš€ Real-world usage:")
print(f"   âœ… This is exactly how you'd use the model in production")
print(f"   âœ… Real estate app: user inputs house details â†’ instant price estimate")
print(f"   âœ… Bank loan approval: quick property valuation")
print(f"   âœ… Investment analysis: evaluate potential purchases")

print(f"\nğŸ’¡ Key points:")
print(f"   âœ… Model works on ANY new house (not just test data)")
print(f"   âœ… Same preprocessing must be applied (VectorAssembler)")
print(f"   âœ… Prediction is instant (no retraining needed)")
print(f"   âœ… Model generalizes beyond the original dataset")

# Let's predict a few more houses to see the model in action
print(f"\nğŸ  Let's predict a few more houses:")

more_houses = spark.createDataFrame([
    (2500, 4, 3, 3),   # Large, new house
    (1200, 2, 1, 15),  # Small, older house  
    (3000, 5, 4, 1)    # Very large, very new house
], ["area", "bedrooms", "bathrooms", "age"])

more_features = assembler.transform(more_houses)
more_predictions = model.transform(more_features)

print("   Batch predictions:")
more_predictions.select("area", "bedrooms", "bathrooms", "age", "prediction").show()

batch_results = more_predictions.select("area", "bedrooms", "bathrooms", "age", "prediction").collect()
for i, row in enumerate(batch_results):
    print(f"   House {i+1}: {row['area']} sqft, {row['bedrooms']} bed â†’ ${row['prediction']:,.0f}")

print("\nâœ… Example 7 complete! Model successfully predicted prices for new houses.")
print("ğŸ’¡ Next: Learn how to wrap everything in a Spark Pipeline for production use!")





# ========================================
# Example 8: Spark Pipeline
# ========================================

# NOTE: This can be run independently, but works best after Examples 1-7
# We'll use the same data but show the PROFESSIONAL way to do ML in Spark

print("ğŸ”„ Building a Spark Pipeline...")

from pyspark.ml import Pipeline

# A Pipeline chains multiple steps together into one workflow
# This is the PRODUCTION-READY way to do machine learning

print("\nğŸ§© What is a Pipeline?")
print("   ğŸ“¦ Bundles preprocessing + modeling into ONE object")
print("   ğŸ”’ Ensures same steps are always applied in same order")
print("   ğŸš€ Makes deployment easier - save/load entire workflow")
print("   ğŸ›¡ï¸ Prevents data leakage and preprocessing errors")

# Create individual pipeline stages (same as before, but organized)
print("\nğŸ—ï¸ Creating pipeline stages...")

# Stage 1: Feature Engineering
assembler = VectorAssembler(
    inputCols=["area", "bedrooms", "bathrooms", "age"],
    outputCol="features"
)
print("   âœ… Stage 1: VectorAssembler (feature engineering)")

# Stage 2: Machine Learning Algorithm  
lr = LinearRegression(
    featuresCol="features", 
    labelCol="price"
)
print("   âœ… Stage 2: LinearRegression (model training)")

# Create the Pipeline by chaining stages together
pipeline = Pipeline(stages=[
    assembler,  # Step 1: Convert columns to features vector
    lr          # Step 2: Apply linear regression
])

print("\nğŸ”— Pipeline created with 2 stages:")
print("   1ï¸âƒ£ VectorAssembler â†’ creates features vector")
print("   2ï¸âƒ£ LinearRegression â†’ learns price prediction")

print("\nğŸ“Š Using the same data from previous examples...")
# We'll use the feature_data from earlier examples, but let's recreate it for clarity
if 'data' not in locals():
    # If running independently, recreate the data
    data = spark.createDataFrame([
        (1500, 2, 1, 10, 300000),
        (2000, 3, 2, 5, 450000),
        (1200, 1, 1, 15, 250000),
        (2500, 4, 3, 2, 600000),
        (1800, 2, 2, 8, 380000),
        (1600, 2, 1, 12, 320000),
        (2200, 3, 2, 6, 480000),
        (1000, 1, 1, 20, 200000)
    ], ["area", "bedrooms", "bathrooms", "age", "price"])

# Split the data
train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)
print(f"   Training samples: {train_data.count()}")
print(f"   Test samples: {test_data.count()}")

# Fit the entire pipeline on training data
print("\nğŸ”„ Training the complete pipeline...")
pipeline_model = pipeline.fit(train_data)

print("âœ… Pipeline training complete!")
print("   ğŸ¯ Both VectorAssembler and LinearRegression are now trained")
print("   ğŸ“¦ Everything bundled into one 'pipeline_model' object")

# Make predictions using the complete pipeline
print("\nğŸ”® Making predictions with pipeline...")
pipeline_predictions = pipeline_model.transform(test_data)

print("âœ… Pipeline predictions complete!")
print("\nğŸ“Š Results:")
pipeline_predictions.select("area", "bedrooms", "bathrooms", "age", "price", "prediction").show()

# Evaluate the pipeline model (same as before)
evaluator = RegressionEvaluator(
    labelCol="price", 
    predictionCol="prediction", 
    metricName="rmse"
)

pipeline_rmse = evaluator.evaluate(pipeline_predictions)
print(f"\nğŸ“ Pipeline RMSE: ${pipeline_rmse:,.2f}")

# The magic of pipelines: predict new data in ONE step
print("\nğŸ© The Pipeline Magic:")
print("   Instead of manually doing:")
print("   1ï¸âƒ£ new_data â†’ VectorAssembler â†’ features")
print("   2ï¸âƒ£ features â†’ LinearRegression â†’ predictions")
print("   ")
print("   Pipeline does EVERYTHING in one step:")
print("   ğŸš€ new_data â†’ Pipeline â†’ predictions")

# Demonstrate with a new house
new_house = spark.createDataFrame([
    (2100, 3, 2, 4)  # 2100 sqft, 3 bed, 2 bath, 4 years old
], ["area", "bedrooms", "bathrooms", "age"])

print(f"\nğŸ  Testing pipeline with new house:")
new_house.show()

# ONE command does everything: preprocessing + prediction
new_house_prediction = pipeline_model.transform(new_house)

print("âœ… Pipeline result (one step!):")
new_house_prediction.select("area", "bedrooms", "bathrooms", "age", "features", "prediction").show()

predicted_price = new_house_prediction.select("prediction").collect()[0][0]
print(f"ğŸ’° Predicted price: ${predicted_price:,.2f}")

print(f"\nğŸ† Benefits of Pipelines:")
print("âœ… NO manual preprocessing - pipeline handles it automatically")
print("âœ… NO risk of forgetting steps or doing them wrong")
print("âœ… CONSISTENT processing between training and prediction")
print("âœ… EASY to save/load entire workflow")
print("âœ… PRODUCTION-READY - scales to millions of records")

# Show how to save/load pipeline (commented out to avoid file system issues)
print(f"\nğŸ’¾ Saving/Loading Pipelines (for production):")
print("   # Save the trained pipeline")
print("   # pipeline_model.write().overwrite().save('house_price_model')")
print("   ")
print("   # Load the pipeline later")
print("   # from pyspark.ml import PipelineModel")
print("   # loaded_model = PipelineModel.load('house_price_model')")
print("   # predictions = loaded_model.transform(new_data)")

print(f"\nğŸ”„ Pipeline vs Manual Approach:")
print("ğŸ“Š Manual (Examples 1-7):")
print("   âŒ Multiple steps, easy to make mistakes")
print("   âŒ Must remember exact preprocessing sequence")
print("   âŒ Risk of train/test preprocessing differences")
print("")
print("ğŸš€ Pipeline (Example 8):")
print("   âœ… One object does everything")
print("   âœ… Impossible to forget preprocessing steps")
print("   âœ… Guaranteed consistency")
print("   âœ… Professional, production-ready approach")

# Compare results to make sure pipeline gives same answer
if 'predictions' in locals():
    manual_rmse = evaluator.evaluate(predictions)
    print(f"\nğŸ¯ Verification:")
    print(f"   Manual approach RMSE: ${manual_rmse:,.2f}")
    print(f"   Pipeline approach RMSE: ${pipeline_rmse:,.2f}")
    if abs(manual_rmse - pipeline_rmse) < 0.01:
        print("   âœ… Results match perfectly!")
    else:
        print("   âš ï¸ Small differences (normal due to randomization)")

print("\nğŸ“ When to use Pipelines:")
print("âœ… ALWAYS use pipelines for production systems")
print("âœ… Use for any serious ML project")  
print("âœ… Use when you need to save/deploy models")
print("âœ… Use to prevent preprocessing mistakes")
print("")
print("ğŸ“š Manual approach is good for:")
print("âœ… Learning and understanding each step")
print("âœ… Debugging and experimentation")
print("âœ… Quick prototyping")

print("\nâœ… Example 8 complete! You now know the professional way to do ML in Spark.")